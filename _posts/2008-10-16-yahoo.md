---
layout: post
title: Экскурсия в "серверную" Yahoo!
date: '2008-10-16T13:00:00.003-05:00'
author: Mikhail Kyraha
tags:
- веб
- опыт
- техно
modified_time: '2011-08-13T14:59:37.862-05:00'
---

Yahoo.com занимает первое место в мире по количеству посетителей в день, судя по измерениям Alexa.com. Т.е. это значит Yahoo! впереди Гугла, Амазона, Фэйсбука и прочих Яндексов и Одноклассников. Впереди, так сказать, планеты всей. Есть, говорят, такие страны даже, где слова Yahoo и Интернет - синонимы. Как думаете, интересно было бы заглянуть в "серверную" такой компании? Мне очень! Читаем дальше...

Итак, вашему покорному слуге, выдалась уникальнейшая возможность побывать внутри, как они сами называют место, где расположены сервера, дэйта-центра (data center) самого большого в мире на сегодня интернет гиганта.  Как выяснилось в ходе экскурсии, это ещё не самый большой дэйта-центр компании, но зато тот который расположен в самом сердце Кремниевой (Силиконовой) Долины, и пожалуй самый старый из всех, которые есть в наличии у компании. Оказывается, размеры дэйтацентров измеряются не столько площадью помещения, как я думал раньше, и как обычно пишут в прессе, а сколько в потребляемой мощности. Этот данный дэйта-центр, например, на пять мегаватт. Как я уже заметил, это ещё не очень большой. Есть, говорят, один где-то в Сингапуре, так тот на 70 мегаватт. Но сегодня речь не о нём.

Значит, основной ресурс, который требуется для работы дэйта-центра, это электроэнергия. Логично. Сервера и телекомуникации, всё должно быть воткнуто в розетку. А что является основным потребителем электроэнергии в дэйта-центре? Оказывается, далеко не сервера (компютеры) и не сетевое оборудование. И даже не хард-драйвы, или так называемые файлеры (filers), как может показаться на первый взгляд. Основную часть счёта на электричество, оказывается, составляют расходы на охлаждение помещения, т.е. на питание кондиционеров. И шум в помещении, а в дэйта-центре очень даже не тихо, создаётся работающими кондиционерами. И затраты на электричество, вобщем-то, даже значительно превышают стоимость оборудования находящегося внутри. Так что, господа экономисты, если кто-то столкнётся с бизнес расчётами создания дэйта-центра, имейте это в виду ;)

Отдельно про охлаждение. Охлаждение оборудования - это один из самых важных моментов в работе таких устройств, как сервера и сетевые дивайсы. Сервера должны работать круглые сутки, семь дней в неделю, 365.25 дней в году. И работать безотказно. Основная причина отказов комптютерной техники, если вы ещё не знаете, это перегрев. Поэтому за температурным режимом в дэйта-центре следаят очень и очень тщательно. Охлаждение воздушное. Холодный воздух от кондиционеров подводится под полом и через железные решётки направляется непосредственно на переднюю часть каждой стойки, т.к. обычно сервера устроены так, что вентиляторы гонят воздух внутри корпуса каждого сервера в направлении от лицевой панели к задней стенке. Поэтому, если подойти к серверной стойке спереди, скажем, чтобы поближе посмотреть на лампочки или почитать надписи, то попадаешь в восходящий поток ледяного воздуха. Довольно приятное ощущение в жаркую погоду. А если обойти ряд стоек сзади, то от них уже дует тёплым воздухом. Можно там погреться, если замёрз. Тёплый воздух поднимается под потолок и там собирается коллекторами тех же кондиционеров. Кстати, система кондиционирования воздуха устроена так, что кроме собственно охлаждения и фильтрации пыли из воздуха, она ещё и поддерживает избыточное давление в помещении, так чтобы пыль с улицы и коридоров не попадала в "машинный зал" через возможные щели в окнах и дверях. Поэтому вентиляционные решётки в корпусах серверов, даже доисторических Пентиумов-III, выглядят совсем как новенькие по сравнению, например с моим домашним компьютером под столом, который я и регулярно пылесошу.

Локальная сеть в дэйта-центре. Полторы тысячи серверных стоек, более сорока тысяч хостов в одном помещении. Это вам не СКС в каком-то там офисе. Кабельная система должна быть не просто структурированная, а очень структурированная. Такой аккуратности в разводке кабелей, маркировке и укладке я не видел ещё нигде. Теперь я смело могу сказать, что я своими глазами видел, как должна выглядеть правильно выполненная структурированная кабельная система и почему она так называется. Во первых, сеть не одна, а как бы, две идентичные, практически нигде не пересекающиеся кабельные системы. Одна дублирект другую. Каждый хост подключен в каждую из этих сетей, для чего они всегда снабжены двумя сетевыми картами. Чтобы не запутаться где какая сеть, везде используются провода двух разных цветов. По стойкам к хостам спускаются жгуты пятой категории жёлтого цвета слева, а лилового - справа. Или наоборот. Наверху каждого шкафа (стойки) обязательно два свича, каждый из которых уже оптическим кабелем соединён с цетральным раутером. На самом деле, это не один раутер, типа коробка с дырочками. А отдельный, как бы это сказать, зал с двумя рядами опять же стоек, напичканых сетевым оборудованием. Там и происходит вся логистика маршрутизации пакетов по нескольким, так называемым, ап-стрим провайдерам. Какие-то из них провайдают соединение с другими дэйта-центрами компании, расположенными в других городах и даже странах. А какие-то, или те же, но по другим проводам, выход во внешний интернет (или вход внешнего интернета в сеть дэйта-центра?)

Кстати, возвращаясь опять к электропитанию, электроэнергия ко всему оборудованию тоже подводится двумя непересекающимися электросетями, от двух независимых компаний-поставщиков электричества. Опять же, для этой цели во всех серверах предусмотрены по два независимых блока питания. Электросети бэкапнуты специальными УПСами (этакие железные шкафы там и сям по залу). Также на улице стоит дизельный генератор с запасом топлива не менее чем на сутки непрерывной работы. И заключён контракт с топливной компанией (а может и не с одной) о доставке необходимого количества солярки по первому требованию.

Пожарная безопасность тоже особенная, не как в обычном офисе или жилом помещении. Детекторы-то да, конечно такие же, а вот спринклеров нет. Т.е. они есть в потолке, но, как нам объяснили, отключены и не используются. На случай пожара предусмотрена другая система огнетушения - всё помещение моментально наполняется толи азотом, толи углекислым газом, а кислород вытесняется наружу. У персонала, если кто-то находился в этот момент в зале, есть ровно столько времени (уже не помню точную цифру) сколько надо, чтобы покинуть помещение. Об этом предупреждают сирены перекрывающие даже шум в зале, а также яркие стробоскопические вспышки на случай, если кто-то надел шумоподавляющие наушники и слушает громкую музыку. Ребята в дэйтацентре, кстати, как раз очень даже пользуются такими. Не знаю что они там слушают, но сколько я их ни видел (человека два-три), все ходят в наушниках.

Весь дэйтацентр обслуживают семь человек. Дежурят круглосуточно сменами по 8 часов. В их повседневные обязанности входит: в первую очередь, быть руками системных администраторов всех тех серверов, собранных в этом дэйтацентре, т.е. если надо ввести в строй новую партию хардвера, или наоборот списать какое-то устаревшее оборудование, перенести/переключить сервера из одного сегмента сети в другой и т.д. ну или даже просто элементарно пойти и нажать кнопку рисет на серевере. Кроме того, хотя это в основном делают автоматы, они всё равно должны следить за состоянием всех систем - энергообеспечения, сетей, климата. Автоматы, конечно, тоже мониторят все эти системы и сообщают обо всех неполадках соответствующим бригадам, но человек всё-таки есть человек.